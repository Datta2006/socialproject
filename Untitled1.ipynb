{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee41ed34-7670-4275-916b-a9ee85e8d3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected text columns -> fake: title , true: title\n",
      "\n",
      "Combined dataset rows: 44898\n",
      "Label distribution:\n",
      "label\n",
      "1    23481\n",
      "0    21417\n",
      "Name: count, dtype: int64\n",
      "\n",
      "First 8 rows (text preview):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ben Stein Calls Out 9th Circuit Court: Committed a ‘Coup d’état’ Against the Constitution</td>\n",
       "      <td>21st Century Wire says Ben Stein, reputable professor from, Pepperdine University (also of some Hollywood fame appearing in TV shows and films such as Ferris Bueller s Day Off) made some provocati...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Trump drops Steve Bannon from National Security Council</td>\n",
       "      <td>WASHINGTON (Reuters) - U.S. President Donald Trump removed his chief strategist Steve Bannon from the National Security Council on Wednesday, reversing his controversial decision early this year t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Puerto Rico expects U.S. to lift Jones Act shipping restrictions</td>\n",
       "      <td>(Reuters) - Puerto Rico Governor Ricardo Rossello said on Wednesday he expected the federal government to waive the Jones Act, which would lift restrictions on ships that can provide aid to the is...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OOPS: Trump Just Accidentally Confirmed He Leaked Israeli Intelligence To Russia (VIDEO)</td>\n",
       "      <td>On Monday, Donald Trump once again embarrassed himself and his country by accidentally revealing the source of the extremely classified information he leaked to Russia earlier this month.While it ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Donald Trump heads for Scotland to reopen a golf resort</td>\n",
       "      <td>GLASGOW, Scotland (Reuters) - Most U.S. presidential candidates go abroad to sharpen their foreign policy credentials. Donald Trump arrives in Scotland on Friday to reopen a golf resort. The presu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Paul Ryan Responds To Dem’s Sit-In On Gun Control In The Most DISGUSTING Way (VIDEO)</td>\n",
       "      <td>On Wednesday, Democrats took a powerful stance against the GOP s refusal to vote on gun control measures by staging a sit-in. While Republican politicians called a recess and took a lunch break, D...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AWESOME! DIAMOND AND SILK Rip Into The Press: “We don’t believe you!” [Video]</td>\n",
       "      <td>President Trump s rally in FL on Saturday was a smashing success with about 9000 crowding into a hanger to hear him speak and another two or three times that amount congregating outside.Trump told...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>STAND UP AND CHEER! UKIP Party Leader SLAMS Germany, France And EU Invasion Of Phony Refugees [VIDEO]</td>\n",
       "      <td>He s been Europe s version of the outspoken Ted Cruz for some time now. Nigel Farage, leader of the UK Independence Party may be the most disliked member of the European Parliment. But he plows ah...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                    text  \\\n",
       "0              Ben Stein Calls Out 9th Circuit Court: Committed a ‘Coup d’état’ Against the Constitution   \n",
       "1                                                Trump drops Steve Bannon from National Security Council   \n",
       "2                                       Puerto Rico expects U.S. to lift Jones Act shipping restrictions   \n",
       "3               OOPS: Trump Just Accidentally Confirmed He Leaked Israeli Intelligence To Russia (VIDEO)   \n",
       "4                                                Donald Trump heads for Scotland to reopen a golf resort   \n",
       "5                   Paul Ryan Responds To Dem’s Sit-In On Gun Control In The Most DISGUSTING Way (VIDEO)   \n",
       "6                          AWESOME! DIAMOND AND SILK Rip Into The Press: “We don’t believe you!” [Video]   \n",
       "7  STAND UP AND CHEER! UKIP Party Leader SLAMS Germany, France And EU Invasion Of Phony Refugees [VIDEO]   \n",
       "\n",
       "                                                                                                                                                                                                      text  \\\n",
       "0  21st Century Wire says Ben Stein, reputable professor from, Pepperdine University (also of some Hollywood fame appearing in TV shows and films such as Ferris Bueller s Day Off) made some provocati...   \n",
       "1  WASHINGTON (Reuters) - U.S. President Donald Trump removed his chief strategist Steve Bannon from the National Security Council on Wednesday, reversing his controversial decision early this year t...   \n",
       "2  (Reuters) - Puerto Rico Governor Ricardo Rossello said on Wednesday he expected the federal government to waive the Jones Act, which would lift restrictions on ships that can provide aid to the is...   \n",
       "3  On Monday, Donald Trump once again embarrassed himself and his country by accidentally revealing the source of the extremely classified information he leaked to Russia earlier this month.While it ...   \n",
       "4  GLASGOW, Scotland (Reuters) - Most U.S. presidential candidates go abroad to sharpen their foreign policy credentials. Donald Trump arrives in Scotland on Friday to reopen a golf resort. The presu...   \n",
       "5  On Wednesday, Democrats took a powerful stance against the GOP s refusal to vote on gun control measures by staging a sit-in. While Republican politicians called a recess and took a lunch break, D...   \n",
       "6  President Trump s rally in FL on Saturday was a smashing success with about 9000 crowding into a hanger to hear him speak and another two or three times that amount congregating outside.Trump told...   \n",
       "7  He s been Europe s version of the outspoken Ted Cruz for some time now. Nigel Farage, leader of the UK Independence Party may be the most disliked member of the European Parliment. But he plows ah...   \n",
       "\n",
       "   label  \n",
       "0      1  \n",
       "1      0  \n",
       "2      0  \n",
       "3      1  \n",
       "4      0  \n",
       "5      1  \n",
       "6      1  \n",
       "7      1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# paths (adjust if your files are in a different folder)\n",
    "fake_path = \"data/fake.csv\"\n",
    "true_path = \"data/true.csv\"\n",
    "\n",
    "# load\n",
    "df_fake = pd.read_csv(fake_path)\n",
    "df_true = pd.read_csv(true_path)\n",
    "\n",
    "# add standardized label: 1 = fake, 0 = real/true\n",
    "df_fake = df_fake.copy()\n",
    "df_true = df_true.copy()\n",
    "df_fake['label'] = 1\n",
    "df_true['label'] = 0\n",
    "\n",
    "# try to find a text column (common names)\n",
    "def find_text_col(df):\n",
    "    for c in df.columns:\n",
    "        if c.lower() in (\"text\",\"content\",\"article\",\"headline\",\"title\",\"body\"):\n",
    "            return c\n",
    "    # fallback: pick the longest-string column\n",
    "    str_cols = [c for c in df.columns if df[c].dtype == object]\n",
    "    if not str_cols:\n",
    "        return None\n",
    "    return max(str_cols, key=lambda col: df[col].astype(str).str.len().median())\n",
    "\n",
    "fake_text_col = find_text_col(df_fake)\n",
    "true_text_col = find_text_col(df_true)\n",
    "\n",
    "print(\"Detected text columns -> fake:\", fake_text_col, \", true:\", true_text_col)\n",
    "\n",
    "# rename to unified column 'text'\n",
    "df_fake = df_fake.rename(columns={fake_text_col: \"text\"})\n",
    "df_true = df_true.rename(columns={true_text_col: \"text\"})\n",
    "\n",
    "# keep only text + label (drop others for now)\n",
    "df_fake = df_fake[[\"text\",\"label\"]].copy()\n",
    "df_true = df_true[[\"text\",\"label\"]].copy()\n",
    "\n",
    "# concat and shuffle\n",
    "df = pd.concat([df_fake, df_true], ignore_index=True).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(\"\\nCombined dataset rows:\", len(df))\n",
    "print(\"Label distribution:\")\n",
    "print(df['label'].value_counts())\n",
    "\n",
    "print(\"\\nFirst 8 rows (text preview):\")\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "display(df.head(8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b33bfa93-4b4b-4a05-92a2-d62b93faff4a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reindex on an axis with duplicate labels",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m     text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124ms+\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m, text)\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text\n\u001b[0;32m---> 13\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclean_text\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mapply(clean)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# split data\u001b[39;00m\n\u001b[1;32m     16\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[1;32m     17\u001b[0m     df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclean_text\u001b[39m\u001b[38;5;124m\"\u001b[39m], \n\u001b[1;32m     18\u001b[0m     df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     stratify\u001b[38;5;241m=\u001b[39mdf[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     22\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py:4311\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4308\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array([key], value)\n\u001b[1;32m   4309\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4310\u001b[0m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[0;32m-> 4311\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_item(key, value)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py:4524\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4514\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_set_item\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4515\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4516\u001b[0m \u001b[38;5;124;03m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[1;32m   4517\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4522\u001b[0m \u001b[38;5;124;03m    ensure homogeneity.\u001b[39;00m\n\u001b[1;32m   4523\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4524\u001b[0m     value, refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sanitize_column(value)\n\u001b[1;32m   4526\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   4527\u001b[0m         key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[1;32m   4528\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   4529\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value\u001b[38;5;241m.\u001b[39mdtype, ExtensionDtype)\n\u001b[1;32m   4530\u001b[0m     ):\n\u001b[1;32m   4531\u001b[0m         \u001b[38;5;66;03m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[1;32m   4532\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py:5263\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   5261\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, Series):\n\u001b[1;32m   5262\u001b[0m         value \u001b[38;5;241m=\u001b[39m Series(value)\n\u001b[0;32m-> 5263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _reindex_for_setitem(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n\u001b[1;32m   5265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_list_like(value):\n\u001b[1;32m   5266\u001b[0m     com\u001b[38;5;241m.\u001b[39mrequire_length_match(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py:12692\u001b[0m, in \u001b[0;36m_reindex_for_setitem\u001b[0;34m(value, index)\u001b[0m\n\u001b[1;32m  12688\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m  12689\u001b[0m     \u001b[38;5;66;03m# raised in MultiIndex.from_tuples, see test_insert_error_msmgs\u001b[39;00m\n\u001b[1;32m  12690\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m value\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mis_unique:\n\u001b[1;32m  12691\u001b[0m         \u001b[38;5;66;03m# duplicate axis\u001b[39;00m\n\u001b[0;32m> 12692\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[1;32m  12694\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m  12695\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincompatible index of inserted column with frame index\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m  12696\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m  12697\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m reindexed_value, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py:12687\u001b[0m, in \u001b[0;36m_reindex_for_setitem\u001b[0;34m(value, index)\u001b[0m\n\u001b[1;32m  12685\u001b[0m \u001b[38;5;66;03m# GH#4107\u001b[39;00m\n\u001b[1;32m  12686\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m> 12687\u001b[0m     reindexed_value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mreindex(index)\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m  12688\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m  12689\u001b[0m     \u001b[38;5;66;03m# raised in MultiIndex.from_tuples, see test_insert_error_msmgs\u001b[39;00m\n\u001b[1;32m  12690\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m value\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mis_unique:\n\u001b[1;32m  12691\u001b[0m         \u001b[38;5;66;03m# duplicate axis\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/series.py:5153\u001b[0m, in \u001b[0;36mSeries.reindex\u001b[0;34m(self, index, axis, method, copy, level, fill_value, limit, tolerance)\u001b[0m\n\u001b[1;32m   5136\u001b[0m \u001b[38;5;129m@doc\u001b[39m(\n\u001b[1;32m   5137\u001b[0m     NDFrame\u001b[38;5;241m.\u001b[39mreindex,  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[1;32m   5138\u001b[0m     klass\u001b[38;5;241m=\u001b[39m_shared_doc_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mklass\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5151\u001b[0m     tolerance\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   5152\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series:\n\u001b[0;32m-> 5153\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mreindex(\n\u001b[1;32m   5154\u001b[0m         index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[1;32m   5155\u001b[0m         method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[1;32m   5156\u001b[0m         copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[1;32m   5157\u001b[0m         level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[1;32m   5158\u001b[0m         fill_value\u001b[38;5;241m=\u001b[39mfill_value,\n\u001b[1;32m   5159\u001b[0m         limit\u001b[38;5;241m=\u001b[39mlimit,\n\u001b[1;32m   5160\u001b[0m         tolerance\u001b[38;5;241m=\u001b[39mtolerance,\n\u001b[1;32m   5161\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/generic.py:5610\u001b[0m, in \u001b[0;36mNDFrame.reindex\u001b[0;34m(self, labels, index, columns, axis, method, copy, level, fill_value, limit, tolerance)\u001b[0m\n\u001b[1;32m   5607\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_multi(axes, copy, fill_value)\n\u001b[1;32m   5609\u001b[0m \u001b[38;5;66;03m# perform the reindex on the axes\u001b[39;00m\n\u001b[0;32m-> 5610\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_axes(\n\u001b[1;32m   5611\u001b[0m     axes, level, limit, tolerance, method, fill_value, copy\n\u001b[1;32m   5612\u001b[0m )\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreindex\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/generic.py:5633\u001b[0m, in \u001b[0;36mNDFrame._reindex_axes\u001b[0;34m(self, axes, level, limit, tolerance, method, fill_value, copy)\u001b[0m\n\u001b[1;32m   5630\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   5632\u001b[0m ax \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis(a)\n\u001b[0;32m-> 5633\u001b[0m new_index, indexer \u001b[38;5;241m=\u001b[39m ax\u001b[38;5;241m.\u001b[39mreindex(\n\u001b[1;32m   5634\u001b[0m     labels, level\u001b[38;5;241m=\u001b[39mlevel, limit\u001b[38;5;241m=\u001b[39mlimit, tolerance\u001b[38;5;241m=\u001b[39mtolerance, method\u001b[38;5;241m=\u001b[39mmethod\n\u001b[1;32m   5635\u001b[0m )\n\u001b[1;32m   5637\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis_number(a)\n\u001b[1;32m   5638\u001b[0m obj \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_reindex_with_indexers(\n\u001b[1;32m   5639\u001b[0m     {axis: [new_index, indexer]},\n\u001b[1;32m   5640\u001b[0m     fill_value\u001b[38;5;241m=\u001b[39mfill_value,\n\u001b[1;32m   5641\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[1;32m   5642\u001b[0m     allow_dups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   5643\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:4429\u001b[0m, in \u001b[0;36mIndex.reindex\u001b[0;34m(self, target, method, level, limit, tolerance)\u001b[0m\n\u001b[1;32m   4426\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot handle a non-unique multi-index!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   4427\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_unique:\n\u001b[1;32m   4428\u001b[0m     \u001b[38;5;66;03m# GH#42568\u001b[39;00m\n\u001b[0;32m-> 4429\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot reindex on an axis with duplicate labels\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   4430\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4431\u001b[0m     indexer, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_indexer_non_unique(target)\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reindex on an axis with duplicate labels"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# cleaning function (same as sentiment model to reuse SPL assets later)\n",
    "def clean(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"http\\\\S+\", \"\", text)\n",
    "    text = re.sub(r\"[^a-z\\\\s]\", \"\", text)\n",
    "    text = re.sub(r\"\\\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "df[\"clean_text\"] = df[\"text\"].astype(str).apply(clean)\n",
    "\n",
    "# split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[\"clean_text\"], \n",
    "    df[\"label\"],\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df[\"label\"]\n",
    ")\n",
    "\n",
    "# vectorizer (TF-IDF)\n",
    "vectorizer_fake = TfidfVectorizer(max_features=7000)\n",
    "X_train_vec = vectorizer_fake.fit_transform(X_train)\n",
    "X_test_vec = vectorizer_fake.transform(X_test)\n",
    "\n",
    "print(\"Train shape:\", X_train_vec.shape)\n",
    "print(\"Test shape:\", X_test_vec.shape)\n",
    "print(\"Fake-news label distribution (train):\")\n",
    "print(y_train.value_counts())\n",
    "print(\"Fake-news label distribution (test):\")\n",
    "print(y_test.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "173d2ca9-17e2-4e31-8d54-289d37bef51e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before fix - columns: ['text', 'text', 'label']\n",
      "Removed duplicate columns.\n",
      "After fix - columns: ['text', 'label']\n",
      "Train shape: (35918, 7000)\n",
      "Test shape: (8980, 7000)\n",
      "Label distribution (train):\n",
      "label\n",
      "1    18785\n",
      "0    17133\n",
      "Name: count, dtype: int64\n",
      "Label distribution (test):\n",
      "label\n",
      "1    4696\n",
      "0    4284\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Fix duplicate columns, then clean & vectorize (single step)\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 1) show current columns\n",
    "print(\"Before fix - columns:\", df.columns.tolist())\n",
    "\n",
    "# 2) remove duplicate column names (keep first occurrence)\n",
    "if df.columns.duplicated().any():\n",
    "    df = df.loc[:, ~df.columns.duplicated()]\n",
    "    print(\"Removed duplicate columns.\")\n",
    "else:\n",
    "    print(\"No duplicate column names found.\")\n",
    "\n",
    "print(\"After fix - columns:\", df.columns.tolist())\n",
    "\n",
    "# 3) define cleaning and apply\n",
    "def clean(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"http\\\\S+\", \"\", text)\n",
    "    text = re.sub(r\"[^a-z\\\\s]\", \"\", text)\n",
    "    text = re.sub(r\"\\\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "df[\"clean_text\"] = df[\"text\"].astype(str).apply(clean)\n",
    "\n",
    "# 4) split & vectorize\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[\"clean_text\"], \n",
    "    df[\"label\"],\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df[\"label\"]\n",
    ")\n",
    "\n",
    "vectorizer_fake = TfidfVectorizer(max_features=7000)\n",
    "X_train_vec = vectorizer_fake.fit_transform(X_train)\n",
    "X_test_vec  = vectorizer_fake.transform(X_test)\n",
    "\n",
    "print(\"Train shape:\", X_train_vec.shape)\n",
    "print(\"Test shape:\", X_test_vec.shape)\n",
    "print(\"Label distribution (train):\")\n",
    "print(y_train.value_counts())\n",
    "print(\"Label distribution (test):\")\n",
    "print(y_test.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cbbecd7-57f3-4ce1-9e52-95fda836e61a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5141425389755011\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      1.00      0.66      4284\n",
      "           1       1.00      0.07      0.13      4696\n",
      "\n",
      "    accuracy                           0.51      8980\n",
      "   macro avg       0.75      0.54      0.40      8980\n",
      "weighted avg       0.76      0.51      0.39      8980\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[4284    0]\n",
      " [4363  333]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# train model\n",
    "fake_model = LogisticRegression(max_iter=2000)\n",
    "fake_model.fit(X_train_vec, y_train)\n",
    "\n",
    "# evaluate\n",
    "y_pred = fake_model.predict(X_test_vec)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f43a48a-e5b7-4660-9c36-6c431757f826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5240534521158129\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.00      0.00      4284\n",
      "           1       0.52      1.00      0.69      4696\n",
      "\n",
      "    accuracy                           0.52      8980\n",
      "   macro avg       0.76      0.50      0.35      8980\n",
      "weighted avg       0.75      0.52      0.36      8980\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  10 4274]\n",
      " [   0 4696]]\n",
      "\n",
      "Saved improved fake_model and fake_vectorizer to artifacts/\n"
     ]
    }
   ],
   "source": [
    "# One-step improvement: TF-IDF with bigrams + more features + class-balanced logistic regression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import joblib\n",
    "\n",
    "# 1) new vectorizer (bigrams too)\n",
    "vectorizer_fake = TfidfVectorizer(max_features=10000, ngram_range=(1,2), max_df=0.95, min_df=5)\n",
    "X_train_vec = vectorizer_fake.fit_transform(X_train)\n",
    "X_test_vec  = vectorizer_fake.transform(X_test)\n",
    "\n",
    "# 2) balanced logistic regression\n",
    "fake_model = LogisticRegression(max_iter=2000, class_weight='balanced', C=1.0)\n",
    "fake_model.fit(X_train_vec, y_train)\n",
    "\n",
    "# 3) evaluate\n",
    "y_pred = fake_model.predict(X_test_vec)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# 4) save improved artifacts for your app\n",
    "joblib.dump(fake_model, \"artifacts/fake_model.joblib\")\n",
    "joblib.dump(vectorizer_fake, \"artifacts/fake_vectorizer.joblib\")\n",
    "print(\"\\nSaved improved fake_model and fake_vectorizer to artifacts/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86d93cb4-7dac-41dd-afb7-d78ba164a3f5",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "CalibratedClassifierCV.__init__() got an unexpected keyword argument 'base_estimator'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 15\u001b[0m\n\u001b[1;32m      9\u001b[0m X_tr_sub, X_val_sub, y_tr_sub, y_val_sub \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[1;32m     10\u001b[0m     X_train_vec, y_train, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.15\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, stratify\u001b[38;5;241m=\u001b[39my_train\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# 2) Calibrate the already-trained model (uses the model as base estimator)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#    We'll fit a calibrated wrapper on a portion of the train data (X_tr_sub)\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m calibrator \u001b[38;5;241m=\u001b[39m CalibratedClassifierCV(base_estimator\u001b[38;5;241m=\u001b[39mfake_model, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprefit\u001b[39m\u001b[38;5;124m'\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     16\u001b[0m calibrator\u001b[38;5;241m.\u001b[39mfit(X_val_sub, y_val_sub)   \u001b[38;5;66;03m# calibrator expects fitted base; using val for calibration\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# 3) Get calibrated probabilities on validation to find best threshold\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: CalibratedClassifierCV.__init__() got an unexpected keyword argument 'base_estimator'"
     ]
    }
   ],
   "source": [
    "# One-step: Calibrate probabilities + find best threshold (max F1) and evaluate\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import precision_recall_curve, f1_score, classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# 1) Create a small validation split from TRAIN (not the test set)\n",
    "X_tr_sub, X_val_sub, y_tr_sub, y_val_sub = train_test_split(\n",
    "    X_train_vec, y_train, test_size=0.15, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "# 2) Calibrate the already-trained model (uses the model as base estimator)\n",
    "#    We'll fit a calibrated wrapper on a portion of the train data (X_tr_sub)\n",
    "calibrator = CalibratedClassifierCV(base_estimator=fake_model, cv='prefit', method='sigmoid')\n",
    "calibrator.fit(X_val_sub, y_val_sub)   # calibrator expects fitted base; using val for calibration\n",
    "\n",
    "# 3) Get calibrated probabilities on validation to find best threshold\n",
    "val_probs = calibrator.predict_proba(X_val_sub)[:, 1]  # prob for class 1 (fake)\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_val_sub, val_probs)\n",
    "\n",
    "# compute F1 for each threshold (note thresholds array length = len(precisions)-1)\n",
    "f1s = 2 * (precisions[:-1] * recalls[:-1]) / (precisions[:-1] + recalls[:-1] + 1e-12)\n",
    "best_idx = np.nanargmax(f1s)\n",
    "best_threshold = thresholds[best_idx]\n",
    "\n",
    "print(\"Best threshold on validation (max F1):\", best_threshold)\n",
    "print(\"Val F1 at best threshold:\", f1s[best_idx])\n",
    "\n",
    "# 4) Evaluate on TEST set using calibrated probs + best_threshold\n",
    "test_probs = calibrator.predict_proba(X_test_vec)[:, 1]\n",
    "y_pred_thresh = (test_probs >= best_threshold).astype(int)\n",
    "\n",
    "print(\"\\nTest accuracy (thresholded):\", accuracy_score(y_test, y_pred_thresh))\n",
    "print(\"\\nClassification Report (thresholded):\")\n",
    "print(classification_report(y_test, y_pred_thresh))\n",
    "print(\"\\nConfusion Matrix (thresholded):\")\n",
    "print(confusion_matrix(y_test, y_pred_thresh))\n",
    "\n",
    "# 5) Save calibrated model and threshold for your app\n",
    "joblib.dump(calibrator, \"artifacts/fake_model_calibrated.joblib\")\n",
    "# store threshold too\n",
    "joblib.dump(best_threshold, \"artifacts/fake_threshold.joblib\")\n",
    "print(\"\\nSaved calibrated model and threshold to artifacts/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bcef3c85-9966-4e5f-a242-89dae85544c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold on validation: 0.5234119939671674\n",
      "Validation F1 at best threshold: 0.6872332642356176\n",
      "\n",
      "Test accuracy: 0.5240534521158129\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.00      0.00      4284\n",
      "           1       0.52      1.00      0.69      4696\n",
      "\n",
      "    accuracy                           0.52      8980\n",
      "   macro avg       0.76      0.50      0.35      8980\n",
      "weighted avg       0.75      0.52      0.36      8980\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  10 4274]\n",
      " [   0 4696]]\n",
      "\n",
      "Saved calibrated model and threshold.\n"
     ]
    }
   ],
   "source": [
    "# One-step: Calibrate probabilities + find best threshold (max F1) and evaluate\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import precision_recall_curve, f1_score, classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# 1) Create a small validation split from TRAIN (not test set)\n",
    "X_tr_sub, X_val_sub, y_tr_sub, y_val_sub = train_test_split(\n",
    "    X_train_vec, y_train, test_size=0.15, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "# 2) Calibrate the already-trained model (older sklearn uses estimator=)\n",
    "calibrator = CalibratedClassifierCV(estimator=fake_model, cv='prefit', method='sigmoid')\n",
    "calibrator.fit(X_val_sub, y_val_sub)\n",
    "\n",
    "# 3) Get calibrated probabilities on validation for best threshold search\n",
    "val_probs = calibrator.predict_proba(X_val_sub)[:, 1]\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_val_sub, val_probs)\n",
    "\n",
    "# F1 at each threshold\n",
    "f1s = 2 * (precisions[:-1] * recalls[:-1]) / (precisions[:-1] + recalls[:-1] + 1e-12)\n",
    "best_idx = np.nanargmax(f1s)\n",
    "best_threshold = thresholds[best_idx]\n",
    "\n",
    "print(\"Best threshold on validation:\", best_threshold)\n",
    "print(\"Validation F1 at best threshold:\", f1s[best_idx])\n",
    "\n",
    "# 4) Evaluate on TEST with best threshold\n",
    "test_probs = calibrator.predict_proba(X_test_vec)[:, 1]\n",
    "y_pred_thresh = (test_probs >= best_threshold).astype(int)\n",
    "\n",
    "print(\"\\nTest accuracy:\", accuracy_score(y_test, y_pred_thresh))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_thresh))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_thresh))\n",
    "\n",
    "# 5) Save calibrated model + threshold\n",
    "joblib.dump(calibrator, \"artifacts/fake_model_calibrated.joblib\")\n",
    "joblib.dump(best_threshold, \"artifacts/fake_threshold.joblib\")\n",
    "\n",
    "print(\"\\nSaved calibrated model and threshold.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a618e4d-dd88-47cd-8a6f-153c9f6a499b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (RF): 0.5240534521158129\n",
      "\n",
      "Classification Report (RF):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.00      0.00      4284\n",
      "           1       0.52      1.00      0.69      4696\n",
      "\n",
      "    accuracy                           0.52      8980\n",
      "   macro avg       0.76      0.50      0.35      8980\n",
      "weighted avg       0.75      0.52      0.36      8980\n",
      "\n",
      "\n",
      "Confusion Matrix (RF):\n",
      "[[  10 4274]\n",
      " [   0 4696]]\n",
      "\n",
      "Saved Random Forest to artifacts/fake_model_rf.joblib\n"
     ]
    }
   ],
   "source": [
    "# Train a RandomForest baseline for fake-news (balanced)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import joblib\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    class_weight='balanced',\n",
    "    max_depth=None,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train (use existing X_train_vec, y_train, X_test_vec, y_test)\n",
    "rf.fit(X_train_vec, y_train)\n",
    "\n",
    "y_pred_rf = rf.predict(X_test_vec)\n",
    "\n",
    "print(\"Accuracy (RF):\", accuracy_score(y_test, y_pred_rf))\n",
    "print(\"\\nClassification Report (RF):\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "print(\"\\nConfusion Matrix (RF):\")\n",
    "print(confusion_matrix(y_test, y_pred_rf))\n",
    "\n",
    "# Save model & (optionally) wrap vectorizer as before\n",
    "joblib.dump(rf, \"artifacts/fake_model_rf.joblib\")\n",
    "print(\"\\nSaved Random Forest to artifacts/fake_model_rf.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9ea2dbb-789e-4e46-b59d-3a18f6ad4979",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjoblib\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# safety: reduce logging spam\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "# Fine-tune DistilBERT for fake-news classification (small sample, CPU-friendly)\n",
    "# Copy-paste and run in your notebook.\n",
    "\n",
    "# 0) Install deps if missing (uncomment if needed)\n",
    "# !pip install -q transformers datasets accelerate evaluate\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import torch\n",
    "\n",
    "# safety: reduce logging spam\n",
    "import logging\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "\n",
    "# 1) prepare data (uses df with columns ['text','label'] already available)\n",
    "# Use a smaller sample to train quickly (adjust sample_size if you have GPU)\n",
    "sample_size = 7000   # total samples to use (train+val); set higher if you have GPU\n",
    "if len(df) > sample_size:\n",
    "    df_small = df.sample(n=sample_size, random_state=42).reset_index(drop=True)\n",
    "else:\n",
    "    df_small = df.copy()\n",
    "\n",
    "# split into train/val/test: keep your existing X_test_vec/y_test for final eval\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df_small['text'].astype(str).tolist(),\n",
    "    df_small['label'].tolist(),\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df_small['label']\n",
    ")\n",
    "\n",
    "print(\"Training samples:\", len(train_texts), \"Validation samples:\", len(val_texts), \"Test samples (existing):\", len(X_test_vec))\n",
    "\n",
    "# 2) Hugging Face dataset and tokenization\n",
    "from datasets import Dataset, load_metric\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_batch(texts):\n",
    "    return tokenizer(texts, truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "train_ds = Dataset.from_dict({\"text\": train_texts, \"label\": train_labels})\n",
    "val_ds   = Dataset.from_dict({\"text\": val_texts,   \"label\": val_labels})\n",
    "\n",
    "train_ds = train_ds.map(lambda x: tokenize_batch(x[\"text\"]), batched=True)\n",
    "val_ds   = val_ds.map(lambda x: tokenize_batch(x[\"text\"]), batched=True)\n",
    "\n",
    "# set format for PyTorch\n",
    "train_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "val_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "# 3) model (binary classification)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# 4) training args (keep small for CPU)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"artifacts/fake_distilbert\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,              # 1-2 epochs for quick run; increase if you have GPU\n",
    "    logging_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    fp16=False,\n",
    "    push_to_hub=False,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# 5) metrics function\n",
    "metric = load_metric(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    f1 = metric.compute(predictions=preds, references=labels, average=\"binary\")[\"f1\"]\n",
    "    return {\"f1\": f1}\n",
    "\n",
    "# 6) Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# 7) Train (this will print progress)\n",
    "trainer.train()\n",
    "\n",
    "# 8) Evaluate on your held-out test set (use raw df test texts)\n",
    "# prepare test texts from your earlier split X_test / y_test (we used clean_text for fake pipeline)\n",
    "# If you have df test split, use that. We'll build test_texts from X_test (which are cleaned) if available.\n",
    "try:\n",
    "    # If you have 'X_test' as raw texts, use it; otherwise derive from df using test indices.\n",
    "    test_texts = X_test.tolist() if 'X_test' in globals() else df['clean_text'].iloc[y_test.index].tolist()\n",
    "    test_labels = y_test.tolist()\n",
    "except Exception:\n",
    "    # fallback: use a small sample of val as test\n",
    "    test_texts = val_texts\n",
    "    test_labels = val_labels\n",
    "\n",
    "# Tokenize test\n",
    "enc = tokenizer(test_texts, truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(enc[\"input_ids\"], attention_mask=enc[\"attention_mask\"])\n",
    "    logits = outputs.logits.detach().cpu().numpy()\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "print(\"\\nDistilBERT Test accuracy:\", accuracy_score(test_labels, preds))\n",
    "print(\"\\nClassification report:\\n\", classification_report(test_labels, preds))\n",
    "print(\"\\nConfusion matrix:\\n\", confusion_matrix(test_labels, preds))\n",
    "\n",
    "# 9) Save fine-tuned model & tokenizer (Trainer.save_model already saved in output_dir)\n",
    "trainer.save_model(\"artifacts/fake_distilbert\")\n",
    "tokenizer.save_pretrained(\"artifacts/fake_distilbert\")\n",
    "print(\"\\nSaved DistilBERT model & tokenizer to artifacts/fake_distilbert\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c56ee4c-7af5-4a8d-9d95-6fe3ea65ab6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting pipeline (this may take a minute)...\n",
      "Accuracy: 0.9407572383073497\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.96      0.94      4284\n",
      "           1       0.96      0.93      0.94      4696\n",
      "\n",
      "    accuracy                           0.94      8980\n",
      "   macro avg       0.94      0.94      0.94      8980\n",
      "weighted avg       0.94      0.94      0.94      8980\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[4099  185]\n",
      " [ 347 4349]]\n",
      "\n",
      "Saved pipeline -> artifacts/fake_model_skl_pipeline.joblib\n"
     ]
    }
   ],
   "source": [
    "# Stronger sklearn pipeline: word + char ngrams + balanced logistic regression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import joblib\n",
    "\n",
    "# Use raw text column (df['text']) not already-vectorized X_train_vec\n",
    "texts = df[\"clean_text\"] if \"clean_text\" in df.columns else df[\"text\"].astype(str)\n",
    "labels = df[\"label\"]\n",
    "\n",
    "# split (we'll create a fresh split to train quickly)\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(texts, labels, test_size=0.2, random_state=42, stratify=labels)\n",
    "\n",
    "word_vect = TfidfVectorizer(ngram_range=(1,2), max_df=0.95, min_df=5, max_features=15000)\n",
    "char_vect = TfidfVectorizer(analyzer='char_wb', ngram_range=(3,5), max_df=0.95, min_df=5, max_features=5000)\n",
    "\n",
    "union = FeatureUnion([(\"word\", word_vect), (\"char\", char_vect)])\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"feats\", union),\n",
    "    (\"clf\", LogisticRegression(max_iter=2000, class_weight=\"balanced\", C=1.0))\n",
    "])\n",
    "\n",
    "print(\"Fitting pipeline (this may take a minute)...\")\n",
    "pipeline.fit(X_tr, y_tr)\n",
    "\n",
    "# evaluate\n",
    "y_pred = pipeline.predict(X_te)\n",
    "print(\"Accuracy:\", accuracy_score(y_te, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_te, y_pred))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_te, y_pred))\n",
    "\n",
    "# save artifacts for app (optional)\n",
    "import os\n",
    "os.makedirs(\"artifacts\", exist_ok=True)\n",
    "joblib.dump(pipeline, \"artifacts/fake_model_skl_pipeline.joblib\")\n",
    "print(\"\\nSaved pipeline -> artifacts/fake_model_skl_pipeline.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d32134e-7996-41af-9856-0c9c7ff30deb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
